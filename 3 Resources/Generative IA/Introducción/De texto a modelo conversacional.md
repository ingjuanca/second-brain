
---
### ğŸ“ŒÂ **Resumen: De texto a modelo conversacional â€“ CÃ³mo se entrena un modelo de lenguaje**

Este documento explica en detalle cÃ³mo se transforma texto en bruto en un modelo de lenguaje conversacional como ChatGPT.

---

### ğŸ§©Â **1. De texto a tokens**

![[Pasted image 20250704094556.png]]

- El texto no se usa directamente para entrenar el modelo, sino que se convierte enÂ **tokens**Â (fragmentos de palabras o palabras completas con un ID numÃ©rico).
- Hay alrededor deÂ **200,000 tokens Ãºnicos**, incluyendo puntuaciÃ³n, espacios y mayÃºsculas/minÃºsculas.
- Ejemplo: â€œHelloâ€ y â€œhelloâ€ son tokens distintos.

---

### ğŸ”¢Â **2. De tokens a vectores (embeddings)**

![[Pasted image 20250704094704.png]]

- Los tokens se convierten enÂ **vectores numÃ©ricos**Â (embeddings) que representan su significado en un espacio multidimensional.

![[Pasted image 20250704094747.png]]

- Palabras relacionadas (como â€œfuegoâ€ y â€œcalorâ€) se ubican cerca en ese espacio.
- Esto permite que el modelo entienda relaciones semÃ¡nticas.

---

### ğŸ§ Â **3. Entrenamiento del modelo base (pre-entrenamiento)**

![[Pasted image 20250704094848.png]]

- Los vectores se usan para entrenar unaÂ **red neuronal con miles de millones de parÃ¡metros**.
- El modelo aprende aÂ **predecir el siguiente token**Â en una secuencia.
- Ejemplo: si el input es â€œEl fuego estÃ¡â€, el modelo predice tokens como â€œardiendoâ€, â€œcalienteâ€, etc., con diferentes probabilidades.
- Este proceso esÂ **costoso y largo**, y da como resultado unÂ **modelo base**Â oÂ _foundation model_.

---

### ğŸ’¬Â **4. Limitaciones del modelo base**

![[Pasted image 20250704095003.png]]

- El modelo base soloÂ **completa texto**, no responde de forma Ãºtil.
- Ejemplo: si preguntas â€œÂ¿CuÃ¡l es el sentido de la vida?â€, podrÃ­a responder con otra pregunta como â€œÂ¿Y por quÃ© existimos?â€.

---

### ğŸ› ï¸Â **5. Ajuste fino (fine-tuning)**

![[Pasted image 20250704095402.png]]

- Para convertirlo en unÂ **asistente Ãºtil**, se realiza un segundo entrenamiento conÂ **conversaciones creadas por humanos**.

![[Pasted image 20250704095904.png]]

- Estas conversaciones enseÃ±an al modelo a:
    - Responder preguntas.
    - Ser amable y evitar respuestas ofensivas.
    - No responder a ciertos temas sensibles.
- Hoy en dÃ­a, parte de este entrenamiento puede hacerse conÂ **datos sintÃ©ticos generados por otros modelos**.

---

### ğŸ¤–Â **6. Resultado final**

- El modelo ajustado se convierte en unÂ **asistente conversacional**Â como ChatGPT.
- Aunque sigue siendo un sistema deÂ **predicciÃ³n de tokens**, el ajuste fino lo hace parecer mÃ¡s inteligente, Ãºtil y amigable.

---