
---

### ⚠️ **Resumen: El problema de las alucinaciones en los modelos de lenguaje**

Este documento aborda uno de los desafíos más conocidos y persistentes en los modelos de lenguaje: las **alucinaciones**.

---

### 🧠 **¿Qué son las alucinaciones?**

![[Pasted image 20250709213857.png]]

- Los modelos de lenguaje **predicen tokens** (fragmentos de texto) basados en lo que es más probable, **no en lo que es verdadero**.
- Esto significa que pueden **inventar información** que suena creíble pero es falsa.

---

### 🛡️ **¿Qué se ha hecho para reducirlas?**

- Los proveedores de modelos han implementado **“guardrails”** (barreras de seguridad) para reducir las alucinaciones:
    - **Filtrado de preguntas** mediante otras IA o reglas específicas.
    - **Entrenamiento con ejemplos** donde el modelo aprende a responder con frases como:  
        _“Lo siento, no puedo responder eso”_ o _“No tengo esa información”_.

---

### 📉 **¿Funcionan estas medidas?**

- Sí, han **reducido la frecuencia** de alucinaciones.
- Pero **no las eliminan por completo**. El modelo aún puede generar respuestas incorrectas.

---

### ✅ **Recomendaciones clave**

- **Nunca asumas que la respuesta es 100% correcta.**
- **Verifica la información**, especialmente si se trata de temas importantes como:
    - Asuntos legales.
    - Declaraciones de impuestos.
    - Información médica o financiera.

---

### 📌 **Conclusión**

Las alucinaciones siguen siendo una **limitación crítica** de los modelos de lenguaje. Aunque han mejorado, **la verificación humana sigue siendo esencial**.

---