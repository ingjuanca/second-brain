
---
### ğŸ§  Â¿QuÃ© es "atenciÃ³n"?

En el contexto de Transformers, _atenciÃ³n_ significa que el modelo **decide a quÃ© palabras del contexto debe prestar mÃ¡s atenciÃ³n** para entender o predecir algo.

Brandon Rohrer lo ilustra con un ejemplo donde una palabra del texto presta atenciÃ³n a las demÃ¡s para decidir quÃ© informaciÃ³n es relevante.

---
## ğŸ“š Ejemplo sencillo

SupÃ³n que tienes esta frase:

> â€œEl gato se sentÃ³ en la alfombraâ€

Ahora quieres procesar la palabra `"sentÃ³"` y entender su contexto.

El modelo se pregunta:

> Â¿A quÃ© otras palabras debo prestar atenciÃ³n para entender bien la palabra "sentÃ³"?

Tal vez:

- Presta mucha atenciÃ³n a **"gato"** (Â¿quiÃ©n se sentÃ³?)
- Un poco a **"alfombra"** (Â¿dÃ³nde se sentÃ³?)
- Muy poca atenciÃ³n a **"el"**, **"en"**, etc.

---
## ğŸ¯ Â¿CÃ³mo se representa eso?

En matemÃ¡ticas (y en cÃ³digo), lo hacemos con **vectores de atenciÃ³n**: una lista de nÃºmeros que representan la "importancia" de cada palabra para una en particular.

Ejemplo simplificado:

```python
# Palabras de la frase 
["el", "gato", "se", "sentÃ³", "en", "la", "alfombra"]  

# AtenciÃ³n para la palabra "sentÃ³" 
[0.05, 0.4, 0.1, 1.0, 0.1, 0.05, 0.3]
```

> Cuanto mayor el nÃºmero, mÃ¡s atenciÃ³n presta "sentÃ³" a esa palabra.

Luego, esos pesos se usan para **combinar la informaciÃ³n** de las otras palabras.

---
### ğŸ§ª Â¿Y cÃ³mo calcula el modelo esos pesos?

AquÃ­ empieza lo interesante (pero lo haremos muy fÃ¡cil). El modelo:

1. Convierte cada palabra en un **vector numÃ©rico** (embedding).
2. Usa fÃ³rmulas matemÃ¡ticas para calcular **quÃ© tan similares son los significados** entre vectores.
3. Esa similitud define cuÃ¡nta atenciÃ³n se da entre palabras.

---

## ğŸ§® El paso matemÃ¡tico real (pero visual)

La fÃ³rmula que usa el modelo para calcular atenciÃ³n es:

```text
Attention(Q, K, V) = softmax(Q Â· Káµ€ / âˆšdâ‚–) Â· V
```

Vamos a descomponerla **visualmente y paso a paso** en las prÃ³ximas secciones.

Pero por ahora, **tÃº quÃ©date con esta idea**:

> El modelo convierte cada palabra en un vector, calcula cuÃ¡nto se parecen entre sÃ­, y usa eso para decidir a cuÃ¡les prestar mÃ¡s atenciÃ³n. Luego combina la informaciÃ³n de esas palabras en funciÃ³n de esos pesos.

---

## ğŸ§© Â¿QuÃ© viene despuÃ©s?

Ahora que entendemos el concepto, el prÃ³ximo paso es:

### â¡ï¸ **Construir el cÃ¡lculo de atenciÃ³n paso a paso con vectores**

- Primero, con nÃºmeros simples (como listas).
    
- Luego, con matrices y cÃ³digo en Python.

---
Vamos a construir paso a paso cÃ³mo funciona **la atenciÃ³n** usando solo **nÃºmeros simples y lÃ³gica**, sin fÃ³rmulas. Imagina que somos niÃ±os jugando con bloques de colores (vectores).

## ğŸ¯ Objetivo

Queremos que una palabra (por ejemplo, `"sentÃ³"`) decida **a quÃ© otras palabras ponerle atenciÃ³n**.

---

## ğŸ”  Frase de ejemplo

Vamos a usar una frase muy corta:
```css
["el", "gato", "sentÃ³"]
```

Vamos a representar cada palabra con un **vector de 2 nÃºmeros** (esto es como decir que cada palabra tiene dos â€œcaracterÃ­sticasâ€).

### ğŸ”¢ Embeddings (vectores de palabras)

|Palabra|Vector (ficticio)|
|---|---|
|el|[1, 0]|
|gato|[0, 1]|
|sentÃ³|[1, 1]|

> âš ï¸ Estos son vectores inventados, solo para que entendamos la idea.

---

## ğŸ‘€ Â¿QuÃ© significa atenciÃ³n?

Queremos que `"sentÃ³"` mire a **todas las palabras** (incluso a sÃ­ misma) y diga:

> Â¿A cuÃ¡les debo prestar atenciÃ³n?

---

## ğŸ”„ Paso 1: Comparar similitud

Vamos a usar una idea muy simple: si dos vectores apuntan en la **misma direcciÃ³n**, significa que son similares.  
Y si son diferentes, entonces no se parecen.

Una forma fÃ¡cil de medir eso (sin fÃ³rmula) es multiplicar sus elementos uno a uno y sumar el resultado. Esto se llama **producto punto**.

### ğŸ§ª CÃ¡lculo a mano

Vamos a calcular cuÃ¡nto se parece `"sentÃ³"` con cada palabra:

---

#### âœ… AtenciÃ³n de `"sentÃ³"` hacia `"el"`

"sentÃ³" â†’ [1, 1]  
"el" â†’ [1, 0]

Multiplicamos:

```text
1Ã—1 + 1Ã—0 = 1
```

---

#### âœ… AtenciÃ³n de `"sentÃ³"` hacia `"gato"`

"gato" â†’ [0, 1]

```text
1Ã—0 + 1Ã—1 = 1
```

---

#### âœ… AtenciÃ³n de `"sentÃ³"` hacia sÃ­ mismo

"sentÃ³" â†’ [1, 1]

```text
1Ã—1 + 1Ã—1 = 2
```

---

### ğŸ§® Resultado intermedio (similitudes):

|Palabra destino|Similitud con â€œsentÃ³â€|
|---|---|
|el|1|
|gato|1|
|sentÃ³|2|

---

## ğŸ”„ Paso 2: Convertir similitudes en pesos de atenciÃ³n

Queremos convertir esos nÃºmeros en **porcentajes**, para que sumen 1 (100%).

Sumamos todo:

CopyEdit

`1 + 1 + 2 = 4`

Dividimos cada uno entre 4:

|Palabra destino|AtenciÃ³n (peso)|
|---|---|
|el|1 / 4 = 0.25|
|gato|1 / 4 = 0.25|
|sentÃ³|2 / 4 = 0.5|

---

### ğŸ¯ Â¿QuÃ© significa esto?

Cuando `"sentÃ³"` quiere entender su contexto:

- Presta **25% de atenciÃ³n** a "el"
- **25% a "gato"**
- **50% a sÃ­ misma**

---

## ğŸ§  ConclusiÃ³n

Acabas de hacer tu **primer cÃ¡lculo de atenciÃ³n manual** sin usar fÃ³rmulas ğŸ‰  
Esto es exactamente lo que hace un Transformer por dentro (con miles de palabras y vectores mÃ¡s largos).

---