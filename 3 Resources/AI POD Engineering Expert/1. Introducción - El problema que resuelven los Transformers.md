
---

El autor inicia con una idea sencilla: **predecir la siguiente palabra en una frase**.  
Esto parece simple, pero en realidad tiene muchos matices si quieres hacerlo bien.

---

### üß† Ejemplo b√°sico: Modelo de predicci√≥n simple

Frase:

```markdown
El gato se sent√≥ en la ____.
```

Un modelo sencillo podr√≠a mirar solo la √∫ltima palabra y decir:

> ‚ÄúEn la‚Ä¶ ¬øqu√© viene despu√©s normalmente?‚Äù  
> Probablemente: ‚Äúsilla‚Äù, ‚Äúalfombra‚Äù, ‚Äúmesa‚Äù‚Ä¶

Este es el comportamiento de un modelo muy b√°sico, como una **cadena de Markov**: solo ve la palabra anterior.

#### ‚ùóProblema: pierde el contexto anterior

Mira esta frase:

```nginx
El gato se sent√≥ en la alfombra. Luego se lami√≥ las patas y ronrone√≥.
```

Si ahora digo:

```markdown
Despu√©s de unos minutos, ____ salt√≥ de la mesa.
```

¬øQu√© palabra va en el espacio?  
Probablemente: **‚Äúel gato‚Äù**, porque lo vimos antes.  
Pero un modelo que solo mira la √∫ltima palabra ("minutos") no lo puede saber.

---
## üí° ¬øC√≥mo mejorarlo?

Queremos un modelo que:

1. Mire **toda la frase anterior**
    
2. Sepa **qu√© palabras son importantes**
    
3. Y que sepa **c√≥mo est√°n relacionadas entre s√≠**
    

üëâ Aqu√≠ es donde aparece la idea de **atenci√≥n**: una t√©cnica para que el modelo mire hacia atr√°s y _decida a qu√© palabras prestar atenci√≥n_, en lugar de tratar a todas por igual.

---

### üìä El modelo de Brandon Rohrer al principio del [paper]([Transformers from Scratch](https://e2eml.school/transformers.html)):

√âl empieza con un modelo de Markov (solo ve una palabra atr√°s) y dice:

> ‚ÄúEsto es limitado. Necesitamos ver m√°s. Necesitamos atenci√≥n.‚Äù

¬°Y eso nos lleva al siguiente bloque!

---

## üß™ Mini actividad: Representar texto como n√∫meros (¬°y aprender Python!)

En IA no podemos trabajar con palabras como "gato", "mesa" directamente. Debemos convertirlas en n√∫meros.

Esto se llama **tokenizaci√≥n**. Ve√°moslo de forma muy simple en Python:

```python
# Primer bloque de Python muy sencillo 
oracion = "el gato se sent√≥ en la alfombra" 
palabras = oracion.split()  

# Creamos un diccionario con n√∫meros √∫nicos 
tokens = {palabra: idx for idx, palabra in enumerate(set(palabras))} 
print(tokens)
```

üîç Resultado:

```python
`{'en': 0, 'gato': 1, 'alfombra': 2, 'el': 3, 'se': 4, 'sent√≥': 5, 'la': 6}`
```

¬øVes? As√≠ comenzamos a representar palabras como n√∫meros. Luego, esos n√∫meros se transforman en **vectores** (como listas con m√°s n√∫meros), que es la base de los embeddings que usan los Transformers.

---
### üîç Paso a paso:

#### 1. `palabras = oracion.split()`

Primero, convertimos una oraci√≥n en una **lista de palabras**:

```python
oracion = "el gato se sent√≥ en la alfombra" 
palabras = oracion.split()
```

‚û°Ô∏è Resultado:

```python
['el', 'gato', 'se', 'sent√≥', 'en', 'la', 'alfombra']
```

---
#### 2. `set(palabras)`

El `set` elimina palabras duplicadas.  
En este caso no hay duplicadas, pero imagina que s√≠ hubiera:

```python
set(palabras)
```

‚û°Ô∏è Resultado:

```python
{'en', 'gato', 'alfombra', 'el', 'se', 'sent√≥', 'la'}
```

‚ö†Ô∏è **Nota:** El `set` no conserva el orden. Esto no siempre importa, pero si deseas mantener el orden original, usar√≠amos otro enfoque (te lo puedo ense√±ar despu√©s).

---
#### 3. `enumerate(set(palabras))`

`enumerate` toma una lista (o conjunto) y devuelve pares `(√≠ndice, elemento)`, como:

```python
[(0, 'en'), (1, 'gato'), (2, 'alfombra'), ...]
```

---
#### 4. `{palabra: idx for idx, palabra in ...}`

Esto es una **comprensi√≥n de diccionario**: una forma abreviada de escribir lo mismo que este c√≥digo:

```python
tokens = {} 
for idx, palabra in enumerate(set(palabras)):     
tokens[palabra] = idx
```

---
### üß† Resultado final:

Tienes un **diccionario que asigna un n√∫mero √∫nico a cada palabra**:

```python
{   
	'en': 0,   
	'gato': 1,   
	'alfombra': 2,   
	'el': 3,   
	'se': 4,   
	'sent√≥': 5,   
	'la': 6 
}
```

Esto es lo que se llama una **tokenizaci√≥n b√°sica**, y es el primer paso para procesar texto en un modelo como Transformer.

---
### üß© Analog√≠a simple:

Imagina que quieres hacer una sopa (modelo de IA), pero los ingredientes vienen con nombres. El modelo no entiende "zanahoria", "papas"‚Ä¶ as√≠ que los etiquetas como:

```python
zanahoria ‚Üí 0   
papa ‚Üí 1   
agua ‚Üí 2
```

Despu√©s vas a usar esos n√∫meros para hacer las cuentas (multiplicar, sumar, calcular atenci√≥n‚Ä¶).
