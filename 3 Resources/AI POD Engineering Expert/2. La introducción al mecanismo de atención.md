
---
### 🧠 ¿Qué es "atención"?

En el contexto de Transformers, _atención_ significa que el modelo **decide a qué palabras del contexto debe prestar más atención** para entender o predecir algo.

Brandon Rohrer lo ilustra con un ejemplo donde una palabra del texto presta atención a las demás para decidir qué información es relevante.

---
## 📚 Ejemplo sencillo

Supón que tienes esta frase:

> “El gato se sentó en la alfombra”

Ahora quieres procesar la palabra `"sentó"` y entender su contexto.

El modelo se pregunta:

> ¿A qué otras palabras debo prestar atención para entender bien la palabra "sentó"?

Tal vez:

- Presta mucha atención a **"gato"** (¿quién se sentó?)
- Un poco a **"alfombra"** (¿dónde se sentó?)
- Muy poca atención a **"el"**, **"en"**, etc.

---
## 🎯 ¿Cómo se representa eso?

En matemáticas (y en código), lo hacemos con **vectores de atención**: una lista de números que representan la "importancia" de cada palabra para una en particular.

Ejemplo simplificado:

```python
# Palabras de la frase 
["el", "gato", "se", "sentó", "en", "la", "alfombra"]  

# Atención para la palabra "sentó" 
[0.05, 0.4, 0.1, 1.0, 0.1, 0.05, 0.3]
```

> Cuanto mayor el número, más atención presta "sentó" a esa palabra.

Luego, esos pesos se usan para **combinar la información** de las otras palabras.

---
### 🧪 ¿Y cómo calcula el modelo esos pesos?

Aquí empieza lo interesante (pero lo haremos muy fácil). El modelo:

1. Convierte cada palabra en un **vector numérico** (embedding).
2. Usa fórmulas matemáticas para calcular **qué tan similares son los significados** entre vectores.
3. Esa similitud define cuánta atención se da entre palabras.

---

## 🧮 El paso matemático real (pero visual)

La fórmula que usa el modelo para calcular atención es:

```text
Attention(Q, K, V) = softmax(Q · Kᵀ / √dₖ) · V
```

Vamos a descomponerla **visualmente y paso a paso** en las próximas secciones.

Pero por ahora, **tú quédate con esta idea**:

> El modelo convierte cada palabra en un vector, calcula cuánto se parecen entre sí, y usa eso para decidir a cuáles prestar más atención. Luego combina la información de esas palabras en función de esos pesos.

---

## 🧩 ¿Qué viene después?

Ahora que entendemos el concepto, el próximo paso es:

### ➡️ **Construir el cálculo de atención paso a paso con vectores**

- Primero, con números simples (como listas).
    
- Luego, con matrices y código en Python.

---
Vamos a construir paso a paso cómo funciona **la atención** usando solo **números simples y lógica**, sin fórmulas. Imagina que somos niños jugando con bloques de colores (vectores).

## 🎯 Objetivo

Queremos que una palabra (por ejemplo, `"sentó"`) decida **a qué otras palabras ponerle atención**.

---

## 🔠 Frase de ejemplo

Vamos a usar una frase muy corta:
```css
["el", "gato", "sentó"]
```

Vamos a representar cada palabra con un **vector de 2 números** (esto es como decir que cada palabra tiene dos “características”).

### 🔢 Embeddings (vectores de palabras)

|Palabra|Vector (ficticio)|
|---|---|
|el|[1, 0]|
|gato|[0, 1]|
|sentó|[1, 1]|

> ⚠️ Estos son vectores inventados, solo para que entendamos la idea.

---

## 👀 ¿Qué significa atención?

Queremos que `"sentó"` mire a **todas las palabras** (incluso a sí misma) y diga:

> ¿A cuáles debo prestar atención?

---

## 🔄 Paso 1: Comparar similitud

Vamos a usar una idea muy simple: si dos vectores apuntan en la **misma dirección**, significa que son similares.  
Y si son diferentes, entonces no se parecen.

Una forma fácil de medir eso (sin fórmula) es multiplicar sus elementos uno a uno y sumar el resultado. Esto se llama **producto punto**.

### 🧪 Cálculo a mano

Vamos a calcular cuánto se parece `"sentó"` con cada palabra:

---

#### ✅ Atención de `"sentó"` hacia `"el"`

"sentó" → [1, 1]  
"el" → [1, 0]

Multiplicamos:

```text
1×1 + 1×0 = 1
```

---

#### ✅ Atención de `"sentó"` hacia `"gato"`

"gato" → [0, 1]

```text
1×0 + 1×1 = 1
```

---

#### ✅ Atención de `"sentó"` hacia sí mismo

"sentó" → [1, 1]

```text
1×1 + 1×1 = 2
```

---

### 🧮 Resultado intermedio (similitudes):

|Palabra destino|Similitud con “sentó”|
|---|---|
|el|1|
|gato|1|
|sentó|2|

---

## 🔄 Paso 2: Convertir similitudes en pesos de atención

Queremos convertir esos números en **porcentajes**, para que sumen 1 (100%).

Sumamos todo:

CopyEdit

`1 + 1 + 2 = 4`

Dividimos cada uno entre 4:

|Palabra destino|Atención (peso)|
|---|---|
|el|1 / 4 = 0.25|
|gato|1 / 4 = 0.25|
|sentó|2 / 4 = 0.5|

---

### 🎯 ¿Qué significa esto?

Cuando `"sentó"` quiere entender su contexto:

- Presta **25% de atención** a "el"
- **25% a "gato"**
- **50% a sí misma**

---

## 🧠 Conclusión

Acabas de hacer tu **primer cálculo de atención manual** sin usar fórmulas 🎉  
Esto es exactamente lo que hace un Transformer por dentro (con miles de palabras y vectores más largos).

---