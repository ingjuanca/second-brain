
---

Este modelo Gemma aquí, que fue cargado en LM Studio, tiene una longitud de contexto de 128,000 tokens, lo cual no está nada mal, y una longitud de salida de alrededor de 8,000 tokens. Eso podría ser un poco mejor, pero poder cargar 128,000 tokens en el contexto cuando enviamos datos a los modelos o cuando hacemos una pregunta, por ejemplo, es realmente bueno para un modelo de código abierto. Y, por supuesto, es probable que solo mejore en el futuro porque eso significa que podemos, por supuesto, incluir cantidades significativas de conocimiento o datos en nuestros prompts. Y mencioné antes que esa sería una tarea en la que esos modelos serían realmente buenos. Por supuesto, también puedes usarlos para preguntas genéricas, como la capital de Francia. La mayoría de los modelos saben eso, incluso si no tienen muchos parámetros. O pedirles generación de código. Modelos de código abierto muy capaces, como este modelo Gemma aquí y todos los muchos otros modelos de código abierto, típicamente lo harán bien, y pueden ser muy útiles para eso porque, ten en cuenta, puedes usarlos incluso si no tienes internet, y no envías ningún dato a esos grandes proveedores, y no les pagas. Así que pueden ser realmente buenos para eso. Pero, por supuesto, son especialmente buenos cuando se trata de extraer información de datos. Y eso, por supuesto, tiene la gran ventaja de que no estás compartiendo esos datos con esos proveedores.

Aquí en LM Studio, puedo, por ejemplo, también subir archivos. Y lo que haré es ir a mi sitio web personal, y allí, abrir este artículo que escribí sobre MCP, Protocolo de Contexto del Modelo, que también está relacionado con la IA, y ahora lo guardaré como un PDF. También podría simplemente copiar y pegar, por supuesto, pero quiero mostrarte que puedes cargar archivos PDF. Así que almacenaré este archivo PDF de MCP en una carpeta y luego lo abriré aquí. Ahora, aprendimos que hay ciertas limitaciones, por ejemplo, hasta cinco archivos y solo ciertos tipos de extensiones de archivos en el momento en que estoy grabando esto. Pero podemos agregar archivos. Así que aquí, agregaré este archivo PDF de MCP, y nuevamente, solo para dejar claro que todo esto está sucediendo localmente, apagaré el wifi nuevamente. Y diré: Resume este artículo adjunto, y procederá a hacerlo. Y nuevamente, todo esto está sucediendo localmente. Está funcionando en tu hardware, que puede que no sea el más rápido, no comparable a esos servidores y centros de datos que poseen esos grandes proveedores, pero aún así es lo suficientemente bueno. Y al menos también funciona si no tienes conexión a internet. Pero aquí, obtuve un buen resumen de mi artículo, y todo eso fue hecho por este modelo de código abierto que se ejecuta localmente.

---

**Resumen:**

El documento describe el uso del modelo Gemma en LM Studio para realizar tareas como el resumen de texto y el análisis de datos. Este modelo tiene una capacidad de contexto de 128,000 tokens y puede generar salidas de hasta 8,000 tokens. Se destaca que los modelos de código abierto, como Gemma, permiten a los usuarios trabajar sin conexión a internet y sin compartir datos con grandes proveedores. Además, se menciona la posibilidad de cargar archivos PDF en LM Studio, lo que facilita la extracción de información y la generación de resúmenes. El proceso se realiza localmente en el hardware del usuario, lo que puede ser menos rápido que los servidores de grandes empresas, pero sigue siendo efectivo.