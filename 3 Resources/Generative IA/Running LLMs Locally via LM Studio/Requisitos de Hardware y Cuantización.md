
---

Entonces, ¿qué modelos de código abierto tenemos? Bueno, como se mencionó antes, muchos modelos son propietarios. Por ejemplo, la mayoría de los modelos de OpenAI son propietarios. A pesar de su nombre, en realidad no son abiertos, como probablemente ya sabes, y eso, por supuesto, significa que no puedes alojarlos tú mismo. El código abierto significa que tienen una licencia que te permite alojarlos tú mismo o que te permite descargarlos y usarlos. Y, en general, esos modelos propietarios simplemente no están disponibles para ti. Incluso si ignoras su licencia, simplemente no obtendrás sus datos. No obtendrás sus parámetros, que, al final, son lo que los define o lo que compone un modelo de lenguaje grande. Es ese conjunto de valores de parámetros entrenados.

![[Pasted image 20250811165946.png]]

Pero hay muchos modelos de código abierto muy populares y poderosos disponibles. Notablemente, por ejemplo, están los modelos LLaMa de Meta, los modelos Gemma de Google, los modelos DeepSeek y muchos otros, como también viste en esa página de Hugging Face que te mostré, porque Hugging Face es un gran lugar para explorar todos esos modelos de código abierto en detalle. Ahora, en Hugging Face, si exploras esos modelos y, por ejemplo, exploras el modelo Google Gemma, pero podría ser cualquier otro modelo también, encontrarás muchos datos sobre ese modelo en esa página. Y también encontrarás que la comunidad ha creado versiones ajustadas de ese modelo y versiones cuantizadas o cuantizaciones basadas en ese modelo. Y eso es una parte importante porque es algo que te ayudará a ejecutar esos modelos en tu máquina local.

Pero para entender eso, necesitamos comprender qué significa exactamente ejecutar un modelo así en tu máquina. Entonces, ¿qué pasa con esta cuantización y por qué es importante? Bueno, los modelos de lenguaje grandes están, al final, definidos por sus parámetros, miles de millones de parámetros que componen estos modelos de lenguaje grandes. Estos son, al final, simplemente números que se derivan durante las etapas de pre-entrenamiento y ajuste fino. Y estos números, al final, controlan cómo se calcularán los próximos tokens probables cuando proporciones alguna entrada a ese modelo. Eso es, al final, en pocas palabras, cómo funcionan estos modelos. Miles de millones de parámetros que combinados definen qué token, qué palabra es probable que se prediga a continuación. Así que tenemos todos estos parámetros, y cada parámetro, al final, es un número. Y para ser precisos, es un número del tipo float16 o float32.

![[Pasted image 20250811170253.png]]

Ahora, si no estás en programación, si no tienes experiencia con eso, eso puede no decirte mucho y está bien. Al final, esto simplemente significa que estamos hablando de números fraccionarios, así que números que tienen un lugar decimal y que están definidos por muchos números, muchos dígitos después de ese lugar decimal, algo como el número que ves aquí. Y esos números solo pueden ser almacenados hasta una cierta precisión por las computadoras. Y la precisión está definida por la parte de 16 o 32 aquí. Cuanto mayor sea el número, mayor será la precisión. Pero cuanto mayor sea el número, más espacio se ocupará. Y es importante entender que para usar un modelo así, para que prediga tokens, debe ser ejecutado. Y eso significa que debe ser cargado en su totalidad en la memoria de tu computadora, preferiblemente en la memoria de tu tarjeta gráfica, tu VRAM, porque esa RAM puede ser accedida mucho más rápido por tu GPU, que es mucho mejor para ejecutar tales modelos de lenguaje grandes que tu CPU. Ahora, para este curso, no es importante entender por qué es así. Si estás interesado en eso, sin embargo, adjunto encontrarás un enlace a un artículo que escribí que explica en detalle por qué esos modelos deberían preferiblemente ejecutarse en tu GPU, no en tu CPU, por qué aún puedes ejecutarlos en tu CPU, pero cuáles serían las desventajas de eso.

Es importante entender, sin embargo, que todos estos parámetros, todos estos números necesitan ser cargados en tu memoria, preferiblemente la VRAM. Sin embargo, esos números ocupan mucho espacio. El 16 y el 32 aquí representan el número de bits que se utilizan en la memoria para almacenar tal número. Y un número de 16 bits, que es el menos preciso de los dos, que es utilizado por muchos modelos, ese número es dos bytes por número. 16 bits son dos bytes. Y eso, por supuesto, significa que, por ejemplo, si tenemos un modelo muy pequeño que solo utiliza mil millones de parámetros, que es extremadamente pequeño para modelos de lenguaje grandes... Para una comparación, ten en cuenta que el modelo DeepSeekR1, que es un modelo de lenguaje grande muy capaz que está disponible como modelo de código abierto, ese modelo tiene 685 mil millones de parámetros. Y, por lo tanto, un modelo que solo tiene mil millones de parámetros es extremadamente pequeño en comparación. Pero incluso un modelo tan pequeño ya necesita alrededor de dos gigabytes de RAM o VRAM si deseas ejecutarlo localmente porque tenemos dos bytes por número float16 por parámetro al final.

Ahora, por supuesto, dos gigabytes de RAM o RAM virtual son definitivamente manejables por la mayoría de las máquinas. Ahora, necesitas un poco más que eso porque no son solo los parámetros. También hay un poco de sobrecarga. Por ejemplo, tus prompts también necesitan ser cargados en la memoria y si incluyen mucho contexto, eso puede ser, de hecho, un buen trozo de datos que necesita ser cargado. Pero aun así, esto es, por supuesto, bastante manejable para la mayoría de las máquinas. Pero este es también un modelo extremadamente pequeño. Si tomas un modelo con 27 mil millones de parámetros, ya estaríamos hablando de alrededor de 54 gigabytes de RAM. Así que rápidamente alcanzamos áreas donde es bastante difícil cargar tales modelos localmente, y estos seguirían siendo modelos pequeños. Y ahí es donde entra en juego la cuantización. Esa es una técnica que puede ayudarnos a reducir la cantidad de espacio que se necesita para cargar un modelo así en la memoria para facilitar su ejecución local o remota en nuestros propios servidores.

Y la idea detrás de la cuantización es que, en promedio, estos valores de parámetros se simplifican con un proceso matemático que reduce su precisión y que, por lo tanto, conduce a que el modelo funcione un poco peor, pero que también lleva a una reducción tremenda en los requisitos de espacio o tamaño. Y es esta conversión a valores enteros menos precisos y mucho más pequeños que puede, por ejemplo, reducir un modelo de mil millones de parámetros para que solo requiera alrededor de 0.5 gigabytes de VRAM en lugar de dos gigabytes. Así que una cuarta parte del tamaño original. Ahora, por supuesto, será un poco peor. Tendrá un rendimiento un poco inferior. Pero las técnicas modernas de cuantización son en realidad bastante buenas para mantener el rendimiento relativamente estable y, al final, ofrecerte muchas más ventajas en términos de espacio requerido y velocidad de ejecución que desventajas en cuanto al rendimiento del modelo. Es, por supuesto, un compromiso, pero típicamente vale la pena.

Y esa es esta parte de cuantización que también encuentras en Hugging Face. Estos son, al final, modelos que se basan en el modelo base que han sido cuantizados para ocupar menos espacio y ser utilizables en máquinas menos capaces. Ahora, en el caso de DeepSeekR1, sigue siendo bastante difícil ejecutarlo localmente, porque incluso si dividiéramos esta cantidad de parámetros aquí por cuatro, aún terminaríamos con un requisito de memoria que probablemente no sea cumplido por la mayoría de las máquinas. Pero eso, por supuesto, es bastante diferente para otros modelos. Por ejemplo, el modelo Google Gemma, este aquí, que tiene 27 mil millones de parámetros, que, cuando se cuantiza, funcionará en la mayoría de las máquinas. Y con eso, si recuerdas esa página de ranking, obtendrías un modelo bastante capaz que puedes ejecutar de forma gratuita localmente. Y puedes, por ejemplo, ejecutarlo localmente en tu MacBook y, por supuesto, también en muchas máquinas con Windows, porque no necesitas tanta memoria para ejecutarlo. Y eso es lo que haremos a continuación. Exploraremos cómo podemos ejecutar tales modelos localmente.

---

**Resumen:**

El documento explica la disponibilidad y el funcionamiento de los modelos de lenguaje de código abierto, destacando que estos modelos pueden ser utilizados de forma gratuita y ejecutados localmente, a diferencia de muchos modelos propietarios que no están disponibles. Se menciona que Hugging Face es un recurso clave para explorar estos modelos, incluyendo ejemplos como los modelos LLaMa de Meta y Gemma de Google. La cuantización se presenta como una técnica que reduce el tamaño de los modelos al simplificar los valores de sus parámetros, lo que permite que modelos más pequeños se ejecuten en máquinas menos potentes. Aunque los modelos de código abierto suelen ser menos capaces que los modelos propietarios, pueden ser adecuados para tareas específicas y ofrecer ventajas en términos de latencia y privacidad de datos. El documento concluye sugiriendo que se explorará cómo ejecutar estos modelos localmente en secciones posteriores.

---
### Notas

| https://maximilian-schwarzmueller.com/articles/llms-gpu-cpu-vram-ram/        |
| ---------------------------------------------------------------------------- |
| https://maximilian-schwarzmueller.com/articles/making-sense-of-quantization/ |

