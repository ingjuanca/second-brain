
---

Ahora que entendemos por qué podríamos querer ejecutar modelos localmente y que podemos hacerlo, que existen modelos de código abierto y que podemos usar técnicas como la cuantización para hacer posible ejecutar incluso modelos un poco más grandes localmente y tal vez incluso acelerar la ejecución y la velocidad de inferencia en el camino, ahora que sabemos todo eso, la pregunta es: ¿cómo podemos ejecutar esos modelos localmente?

Bueno, en Hugging Face, por ejemplo, tienes un botón de Usar Este Modelo, incluso si no estás conectado. Y allí, encontrarás formas de ejecutar este modelo. Por ejemplo, a través de la biblioteca Transformers ofrecida por Hugging Face, que sería una forma programática de usar un modelo así localmente. Eso es algo que exploraremos un poco más adelante en el curso, sin embargo.

Pero también puedes ejecutarlo con algunas aplicaciones locales. Por ejemplo, LM Studio y Ollama son opciones populares. Y aunque tengo Ollama marcado aquí, en el momento en que estoy grabando esto, no me lo muestra como una opción aquí. Pero puedo decirte que Ollama sería una opción para ejecutar este modelo localmente, porque en general existen diferentes soluciones que hacen que ejecutar modelos de lenguaje grandes de código abierto localmente sea muy fácil.

Notablemente, LM Studio, que diría que es la opción más fácil de usar, Ollama, Llama.cpp y algunas otras opciones. Y también puedes usar modelos de código abierto programáticamente, como mencioné, a través de Ollama, por ejemplo, pero también a través de la biblioteca Transformers de Hugging Face, que vimos. Pero volveré a eso más adelante en el curso cuando exploremos cómo usar modelos de lenguaje grandes programáticamente en tus propias aplicaciones o flujos de trabajo.

Ahora, en esta sección aquí, quiero mostrarte LM Studio como una posible forma de ejecutar modelos de lenguaje grandes localmente. Pero como se mencionó, también puedes explorar el uso de Ollama o Llama CPP. Este último, Llama CPP, es la opción más complicada, diría yo. Es un poco más difícil de usar localmente si no sabes nada sobre programación y el uso de herramientas como Docker y cosas así. Las otras dos opciones son bastante utilizables incluso sin tener conocimientos de programación. Y especialmente LM Studio es realmente una buena aplicación de escritorio que está disponible para todos los sistemas operativos, lo que facilita ejecutar esos modelos localmente y chatear con esos modelos localmente.

Ahora, siempre puedes explorar el catálogo de modelos de estas diferentes opciones. Normalmente lo muestran en algún lugar de su sitio web. Y, por ejemplo, para LM Studio, en el momento en que estoy grabando esto, veo que puedo usar estos modelos Gemma, pero también algunos otros modelos como versiones que están basadas en DeepSeekR1. Aunque no es el grande, estos son, al final, versiones más pequeñas de ese, modelos más pequeños que han sido ajustados basados en datos que fueron producidos por DeepSeekR1, podrías decir. Eso es lo que significa esta parte de Distil aquí.

Pero simplemente comencemos con LM Studio, y para eso, debes descargarlo. Como se mencionó, está disponible para todos los sistemas operativos. Debería seleccionar automáticamente tu sistema por defecto aquí, pero, por supuesto, puedes cambiar eso si no lo hace. Y luego es una aplicación que descargas. Ahora, una vez que la descarga haya terminado, puedes seguir el instalador. Aquí, estoy pasando por el paso de instalación de MacOS, que es realmente simple. Y una vez que hayas hecho eso, puedes abrir esta aplicación LM Studio.

Ahora, el aspecto exacto y las características exactas, por supuesto, pueden cambiar con el tiempo. En general, debería verse algo así. Y, por ejemplo, en el momento en que estoy grabando esto, también puedo cambiar si quiero usarlo en modo usuario, modo usuario avanzado o modo desarrollador, lo que simplemente cambia las herramientas que tengo disponibles. Como desarrollador, puedo, por ejemplo, inspeccionar qué solicitudes puedo enviar a alguna API que se está ejecutando detrás de escena si quisiera interactuar con LM Studio programáticamente y, por lo tanto, con esos modelos que se ejecutan localmente programáticamente. Pero aquí, me quedaré en el modo normal de usuario.

Ahora, lo que necesitarás hacer como primer paso es seleccionar o cargar un modelo. Ahora, ya he cargado algunos modelos aquí. Si no tienes ninguno, ya podrías ver un mensaje emergente que te pide seleccionar un modelo. Pero incluso si tienes algunos modelos, siempre puedes cargar más, por supuesto. Y puedes hacerlo yendo a la configuración, en mi caso aquí en la esquina inferior derecha. Allí también puedes, en general, ajustar la apariencia de esta aplicación, por supuesto, y puedes ir a Búsqueda de Modelos allí. Y este es el lugar donde puedes navegar a través de todos los modelos compatibles e instalarlos de manera conveniente. Por ejemplo, aquí, los modelos Gemma, pero también muchos otros modelos. Y puedes simplemente jugar con ellos. Todos son gratuitos para usar.

Aquí, quiero usar este modelo Gemma 12B, modelo de 12 mil millones de parámetros. Y puedes ver el tamaño que tomará descargarlo aquí en la esquina inferior derecha. También encontrarás una explicación de ese modelo aquí. Y al final, por supuesto, también puedes visitar su tarjeta de modelo en Hugging Face. Así que eso te llevará a la página de Hugging Face de este modelo para aprender más sobre él. Vale la pena señalar que aquí estoy en la página de Hugging Face de la versión ajustada de LM Studio del modelo Gemma 3, y ese es el modelo después de haber pasado por alguna cuantización para hacerlo un poco más pequeño para tu sistema, porque como puedes ver, a pesar de ser un modelo de 12 mil millones de parámetros, solo ocupa ocho gigabytes de tamaño cuando lo descargo aquí, porque estamos usando esta versión cuantizada de él, que es una versión de 4 bits, como te dice esta parte aquí. Así que ocupará menos espacio y memoria que el modelo completo.

Y eso es, por supuesto, algo bueno de tener, aunque en realidad puedes cambiar esas opciones y, por ejemplo, también optar por la opción de 8 bits, que es un poco más precisa, que sería más grande, también cuando se carga en memoria, por supuesto. Y por lo tanto, ahora haré clic en Descargar y eso comenzará la descarga y volveré una vez que haya terminado.

Así que esa descarga ahora terminó para mí, y como siguiente paso, ahora podemos cargar este modelo, ya sea haciendo clic en este botón aquí o simplemente aquí en esta ventana de chat en la parte superior. Cargarlo. Ahora, en la esquina inferior derecha, ves la cantidad de recursos que están siendo utilizados por este modelo. Y para mí aquí, estoy en un Mac, un Mac M1, está cargado en RAM porque este Mac tiene una llamada memoria unificada, lo que significa que la memoria gráfica, la VRAM y la RAM son esencialmente las mismas o compartidas. De cualquier manera, también puedes ejecutar estos modelos incluso si están cargados en RAM, incluso si no tienes una tarjeta gráfica que pudiera ejecutarlos o cargarlos. Simplemente será un poco más lento, y adjunto encontrarás un artículo que compara ejecutar modelos en tu CPU con RAM versus GPU con VRAM. Y también puedes cargarlo parcialmente, poner algunos de los parámetros en VRAM, otros en tu RAM principal si no tienes suficiente VRAM, pero eso no es demasiado importante aquí. Lo importante es que el modelo se cargó y ahora podemos crear un nuevo chat aquí, y ahora estamos chateando con este modelo localmente en nuestra máquina. Puedo apagar mi wifi ahora y funcionará. Así que si pregunto cuál es la capital de Francia, por ejemplo, y envío esto, obtengo una respuesta que resulta ser correcta. Ahora volveré a encender el wifi porque quiero mostrarte algo, algo que vale la pena mencionar y saber.

---

**Resumen:**

El documento describe cómo ejecutar modelos de lenguaje grandes (LLMs) localmente utilizando herramientas y aplicaciones como LM Studio y Ollama. Se menciona que Hugging Face es un recurso clave para encontrar y utilizar modelos de código abierto, y se discute la importancia de la cuantización, que permite reducir el tamaño de los modelos para facilitar su ejecución en máquinas menos potentes. LM Studio se presenta como una opción amigable para usuarios que desean ejecutar modelos localmente, y se explica el proceso de descarga e instalación de modelos, así como la gestión de recursos de memoria. Además, se destaca que los modelos pueden ser utilizados sin conexión a internet una vez que están cargados.